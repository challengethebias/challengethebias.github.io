<!DOCTYPE HTML>
<!--
	Story by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Challenge The Bias</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper" class="divided">

				<!-- One -->
					<section class="banner style1 orient-left content-align-left image-position-right fullscreen onload-image-fade-in onload-content-fade-right">
						<div class="content">
							<h1>Challenge The Bias</h1>
							<p class="major"><!--[-->Deciphering the biases in current algorithms that help decide bail, sentences and parole of a defendant.<!--]--></p>
							<ul class="actions vertical">
								<li><a href="#first" class="button big wide smooth-scroll-bottom">Get Started</a></li>
							</ul>
						</div>
						<div class="image">
							<img src="images/banner.jpg" alt="" />
						</div>
					</section>

				<!-- Two -->
					<section class="spotlight style1 orient-right content-align-left image-position-center onscroll-image-fade-in" id="first">
						<div class="content">
							<h2>The Problem</h2>
                            <p>Judges are constantly making decisions about whether defendants should be released or detained while awaiting a trial. <br><br> How fair are these rulings? <br><br> Human beings are easily biased, and studies have suggested that external factors (like a <a href="http://www.pnas.org/content/108/17/6889.full">lunch break</a> or a <a href="http://www.nber.org/papers/w22611?utm_campaign=ntw&utm_medium=email&utm_source=ntw">tough football loss</a>) can sway their decisions.  In the age of big data, it’s tempting to imagine that using a computer to make rulings might help counteract our own biases.  In fact, “risk scores” generated by algorithms are used nationwide <a href="http://www.law.nyu.edu/sites/default/files/upload_documents/Angele%20Christin.pdf">[1]</a><a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-clinpsy-021815-092945">[2]</a>.  When used as black boxes, however, algorithms are no better (and maybe much worse) in the biases they propagate. A recent study of defendants in Broward County, Florida showed that Black defendants are far more likely to be assigned a high-risk score <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">[3]</a>.</p>
						</div>
						<div class="image">
							<img src="images/spotlight01.jpg" alt="" />
						</div>
					</section>

				<!-- Three -->
					<section class="spotlight style1 orient-left content-align-left image-position-center onscroll-image-fade-in">
						<div class="content">
							<h2>The Users</h2>
                            <p>Data contains valuable information, but we need to understand how to interpret it, use it, and recognize its consequences. We believe that taking the time to understand the effects of using these risk scores with different thresholds will allow <b>judges, lawyers and policy-makers</b> to use data-driven models to make less biased decisions in the criminal justice system.</p>
						</div>
						<div class="image">
							<img src="images/spotlight02.jpg" alt="" />
						</div>
					</section>

				<!-- Four -->
					<section class="spotlight style1 orient-right content-align-left image-position-center onscroll-image-fade-in">
						<div class="content">
							<h2>Current Models</h2>
							<p>Currently, the proprietary <a href="http://www.northpointeinc.com/files/downloads/FAQ_Document.pdf">COMPAS</a> risk assessment score is widely used in court, but evidence shows that this score is suboptimal for minorities and women, and there is currently no system to assess how to make decisions based on a score -- it is simply presented as a number to the judge.  </p>
						</div>
						<div class="image">
							<img src="images/spotlight03.jpg" alt="" />
						</div>
					</section>
                    
                    <!-- Six -->
                    <section class="wrapper style1 align-center">
                        <div class="inner">
                            <h2>Factors</h2>
                            <p>Variables to consider when calculating a risk score</p>
                            <div class="items style1 medium onscroll-fade-in">
                                <section>
                                    <span class="icon style2 major fa-intersex"></span>
                                    <h3>Gender</h3>
                                    <p></p>
                                </section>
                                <section>
                                    <span class="icon style2 major fa-user-plus"></span>
                                    <h3>Age</h3>
                                    <p></p>
                                </section>
                                <section>
                                    <span class="icon style2 major fa-users"></span>
                                    <h3>Race</h3>
                                    <p></p>
                                </section>
                                <section>
                                    <span class="icon style2 major fa-line-chart"></span>
                                    <h3>COMPAS recidivism score</h3>
                                    <p></p>
                                </section>
                                <section>
                                    <span class="icon style2 major fa-bar-chart"></span>
                                    <h3>COMPAS violent recidivism score</h3>
                                    <p></p>
                                </section>
                            </div>
                        </div>
                    </section>

				<!-- Five -->
					<section class="wrapper style1 align-center">
						<div class="inner">
							<h2>What is Fair?</h2>
                            <p>
                            
                            <br><br><b>“Fair”</b> is a word we often throw around, but determining what is the most fair decision involves a lot of tricky tradeoffs to think about.  We consider three types of fairness, and compare how models can be interpreted in each framework.
                            
                            <br><br><b>Equal Thresholds:</b>  Given an algorithmically-generated risk score, we say that any two people with the same risk score have the same ruling.  For example, we could decide that any defendant, regardless of race, gender, or other factor, will be detained if their risk score is about 0.6.
                            
                            <br><br><b>Equal Detention Rates:</b>  Given two populations (i.e. male and female, or black and white), we want to detain an equal rate of people from both populations.  This necessarily means we want different thresholds for different populations.
                           
                           <br><br><b>Equal False Positive Rates:</b> Given two populations, we want to choose thresholds per population such that we enforce equal false positive rates (FPR = the fraction of people who did not reoffend who were detained wrongfully).
                            </p>
						</div>

						<!-- Gallery -->
							<div class="gallery style2 medium lightbox onscroll-fade-in">
								<article>
									<a href="images/gallery/fulls/first.jpg" class="image">
										<img src="images/gallery/thumbs/01.jpg" alt="" />
									</a>
									<div class="caption">
                                        <h3>Gender Risk</h3>
										<p>It is important for risk scores that judges will use to make decisions accurately reflect true risk. We see here results from the COMPAS method which doesn't account for gender: for a given risk score, males are more likely to recidivate than females - this is clearly unfair</p>
										<ul class="actions">
											<li><span class="button small"><a href="https://sjc266.shinyapps.io/gender_fairness/">Details</a></span></li>
										</ul>
									</div>
								</article>
								<article>
									<a href="images/gallery/fulls/second.jpg" class="image">
										<img src="images/gallery/thumbs/02.jpg" alt="" />
									</a>
									<div class="caption">
										<h3>Accounting for Gender</h3>
										<p>A good risk score model will accurately determine the risk of recidivism; this is an example of a model that accounts for gender and now accurately reflects the true risk. However, are we comfortable including gender in a model? We want our data to allow policy makers to be able to make these decisions</p>
                                        <ul class="actions">
                                            <li><span class="button small"><a href="https://sjc266.shinyapps.io/gender_fairness/">Details</a></span></li>
                                        </ul>
									</div>
								</article>
								<article>
									<a href="images/gallery/fulls/third.jpg" class="image">
										<img src="images/gallery/thumbs/03.jpg" alt="" />
									</a>
									<div class="caption">
										<h3>Equal Detention Rates</h3>
										<p>Given two populations (i.e. male and female, or black and white), we want to detain an equal rate of people from both populations. This necessarily means we want different thresholds for different populations</p>
                                        <ul class="actions">
                                            <li><span class="button small"><a href="https://sjc266.shinyapps.io/gender_fairness/">Details</a></span></li>
                                        </ul>
									</div>
								</article>
								<article>
									<a href="images/gallery/fulls/fourth.jpg" class="image">
										<img src="images/gallery/thumbs/04.jpg" alt="" />
									</a>
									<div class="caption">
										<h3>Equal False Positive Ratio</h3>
										<p>Given two populations, we want to choose thresholds per population such that we enforce equal false positive rates (FPR = the fraction of people who did not reoffend who were detained wrongfully)</p>
                                        <ul class="actions">
                                            <li><span class="button small"><a href="https://sjc266.shinyapps.io/gender_fairness/">Details</a></span></li>
                                        </ul>
									</div>
								</article>
								<article>
									<a href="images/gallery/fulls/fifth.jpg" class="image">
										<img src="images/gallery/thumbs/05.jpg" alt="" />
									</a>
									<div class="caption">
										<h3>Equal Thresholds</h3>
										<p>Given an algorithmically-generated risk score, we say that any two people with the same risk score have the same ruling. For example, we could decide that any defendant, regardless of race, gender, or other factor, will be detained if their risk score is about 0.6</p>
                                        <ul class="actions">
                                            <li><span class="button small"><a href="https://sjc266.shinyapps.io/gender_fairness/">Details</a></span></li>
                                        </ul>
									</div>
								</article>
							</div>

					</section>
                    <section class="banner style1 content-align-center">
                        <div class="content">
                            <h2>Solution and Impact</h2>
                            <p><br>To make an unbiased decision based on a computed risk score, the three steps to consider:
                            
                            <br><br>1.) Data collection
                            <br>2.) Data modelling (generating a risk score)
                            <br>3.) Optimal decision-making based on risk score
                            
                            <br><br>We address the second and third points, showing that adding certain variables to an algorithm can make it more fair and how to make optimal decisions based on different concepts of fairness. <b>By creating interpretable visualizations of these concepts, we hope to make fair, data-driven models easier to understand and adopt.</b>  </p>
                        </div>
                        </section>
                    
				<!-- Footer -->
					<footer class="wrapper style1 align-center">
							<p>&copy; Challenge The Bias: <a href="https://html5up.net">challengethebias.org</a></p>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
